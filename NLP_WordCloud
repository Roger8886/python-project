#txt檔案路徑
txt_file_path = './technology_news.txt'

# 載入檔案到變數中
with open(txt_file_path, 'r') as fn:
    lines = fn.readlines()
    lines = list(map(lambda x: x.strip(), lines)) #移除斷行字元

import jieba
#載入字典檔
jieba.load_userdict('./dict.txt.big')

#精確模式斷詞
token_1 = list(map(lambda x: list(jieba.cut(str(x), HMM = False)), lines))
#全模式斷詞
token_2 = list(map(lambda x: list(jieba.cut(str(x), cut_all = True, HMM = False)), lines))
#搜尋引擎模式斷詞
token_3 = list(map(lambda x: list(jieba.cut_for_search(str(x), HMM = False)), lines))

#將斷詞後的結果進行詞頻的計算存入 word_count 變數中，並且篩選出出現次數大於 5 次的字詞
word_count = {}
#計算詞頻
for sent in token_1:
    for word in sent:
        if word not in word_count:
            word_count[word] = 0
        word_count[word] += 1
#篩選出出現次數大於5的字詞
word_count = {word: count for word, count in word_count.items() if count>5}

#利用 wordcloud 套件將剛剛整理好的資料製作成文字雲圖
import wordcloud
from wordcloud import WordCloud
import matplotlib
import matplotlib.pyplot as plt

wordcloud = WordCloud(
    background_color = 'white',
    font_path = 'SourceHanSerifK-Light.otf', #放入中文字型檔路徑
    colormap = matplotlib.cm.cubehelix,
    width = 1600,
    height = 800,
    margin = 2
)
#wordcloud 套件 Input 需放入詞頻統計的 dict 型態變數
wordcloud = wordcloud.generate_from_frequencies(word_count)
plt.figure(figsize=(10,5), facecolor='k')
plt.imshow(wordcloud)
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()
