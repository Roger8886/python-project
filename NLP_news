import jieba

#txt檔案路徑
txt_file_path = './news_01.txt'
# 載入檔案到變數中
with open(txt_file_path, 'r') as fn:
    lines = fn.readlines()
    lines = list(map(lambda x: x.strip(), lines)) #移除斷行字元

jieba.load_userdict('./dict.txt.big')

#精確模式斷詞
token_1 = list(map(lambda x: list(jieba.cut(str(x), HMM = False)), lines))
#全模式斷詞
token_2 = list(map(lambda x: list(jieba.cut(str(x), cut_all = True, HMM = False)), lines))
#搜尋引擎模式斷詞
token_3 = list(map(lambda x: list(jieba.cut_for_search(str(x), HMM = False)), lines))
#計算每個字詞的出現次數 詞頻
word_count={}
for sent in token_1:
    for word in sent:
        if word not in word_count:
            word_count[word] = 0
        word_count[word] += 1

print(word_count)
